{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe76e35d-5cb3-4df2-b6e6-b77d22a32b0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Mosaic AI Agent Framework: Author and deploy a Stateful Agent with Long-term memory using Databricks Lakebase as a Store\n",
    "This notebook demonstrates how to build a stateful agent that stores and retrieves user preference using the Mosaic AI Agent Framework with Lakebase as the agentâ€™s memory store\n",
    "\n",
    "In this notebook, you will:\n",
    "1. Author a Long-term memory Agent graph with Lakebase which stores and recalls users' preferences (via semantic search in store)\n",
    "2. Wrap the LangGraph agent with `ResponsesAgent` interface to ensure compatibility with Databricks features\n",
    "3. Test the agent's behavior locally\n",
    "4. Register model to Unity Catalog, log and deploy the agent for use in apps and Playground\n",
    "\n",
    "## Prerequisites\n",
    "- Have a Lakebase instance ready and running, see Databricks documentation ([AWS](https://docs.databricks.com/aws/en/oltp/create/) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/oltp/create/)). \n",
    "- You can create a Lakebase instance by going to SQL Warehouses -> Lakebase Postgres -> Create database instance. You will need to retrieve values from the \"Connection details\" section of your Lakebase to fill out this notebook.\n",
    "- Complete all the \"TODO\"s throughout this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76004990-0c77-4b94-8043-2600a898cec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cffbc01b-6c68-4c14-9b02-5166c0a2aea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq uv databricks-agents mlflow-skinny[databricks] databricks-langchain[memory]\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc5b35cd-2209-4c7a-a62e-2f61b707efb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## First time setup only: Set up store tables for your Lakebase instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc737d0-dc92-4394-b37c-50a975d6bf82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain import DatabricksStore\n",
    "\n",
    "# TODO: Fill in your Lakebase config values\n",
    "LAKEBASE_INSTANCE_NAME = \"lakebase-name\"\n",
    "\n",
    "store = DatabricksStore(instance_name=LAKEBASE_INSTANCE_NAME)\n",
    "store.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62543b14-2596-4cae-8dda-25735513c037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Define the agent in code\n",
    "\n",
    "## Write agent code to file agent.py\n",
    "Define the agent code in a single cell below. This lets you write the agent code to a local Python file, using the `%%writefile` magic command, for subsequent logging and deployment.\n",
    "\n",
    "## Wrap the LangGraph agent using the ResponsesAgent interface\n",
    "For compatibility with Databricks AI features, the `LangGraphResponsesAgent` class implements the `ResponsesAgent` interface to wrap the LangGraph agent.\n",
    "\n",
    "Databricks recommends using `ResponsesAgent` as it simplifies authoring multi-turn conversational agents using an open source standard. See MLflow's [ResponsesAgent documentation](https://www.mlflow.org/docs/latest/llms/responses-agent-intro/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d541083-879f-4c04-baf4-6811811f4fd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import Annotated, Any, Generator, Optional, Sequence, TypedDict\n",
    "\n",
    "import mlflow\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    DatabricksStore,\n",
    "    UCFunctionToolkit,\n",
    ")\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AIMessageChunk,\n",
    "    BaseMessage,\n",
    ")\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=os.getenv(\"LOG_LEVEL\", \"INFO\"))\n",
    "\n",
    "\n",
    "############################################\n",
    "# Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "# TODO: Replace with your model serving endpoint\n",
    "LLM_ENDPOINT_NAME = \"databricks-claude-3-7-sonnet\"\n",
    "\n",
    "# TODO: Update with your system prompt\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant. Use the available tools to answer questions.\"\n",
    "\n",
    "# TODO: Fill in values for your lakebase instance for agent to use here\n",
    "LAKEBASE_INSTANCE_NAME = \"lakebase-name\"\n",
    "\n",
    "# TODO: Update with your desired embedding configuration values for semantic memory search\n",
    "# Example Model Serving endpoint for text embeddings https://docs.databricks.com/aws/en/machine-learning/foundation-model-apis/supported-models#gte-large-en\n",
    "EMBEDDING_ENDPOINT = \"databricks-gte-large-en\"  \n",
    "EMBEDDING_DIMS = 1024\n",
    "\n",
    "###############################################################################\n",
    "## Define tools for your agent,enabling it to retrieve data or take actions\n",
    "## beyond text generation\n",
    "## To create and see usage examples of more tools, see\n",
    "## https://docs.databricks.com/en/generative-ai/agent-framework/agent-tool.html\n",
    "###############################################################################\n",
    "\n",
    "tools = []\n",
    "\n",
    "# Example UC tools; add your own as needed\n",
    "UC_TOOL_NAMES: list[str] = []\n",
    "if UC_TOOL_NAMES:\n",
    "    uc_toolkit = UCFunctionToolkit(function_names=UC_TOOL_NAMES)\n",
    "    tools.extend(uc_toolkit.tools)\n",
    "\n",
    "# Use Databricks vector search indexes as tools\n",
    "# See https://docs.databricks.com/en/generative-ai/agent-framework/unstructured-retrieval-tools.html#locally-develop-vector-search-retriever-tools-with-ai-bridge\n",
    "# List to store vector search tool instances for unstructured retrieval.\n",
    "VECTOR_SEARCH_TOOLS = []\n",
    "\n",
    "# To add vector search retriever tools,\n",
    "# use VectorSearchRetrieverTool and create_tool_info,\n",
    "# then append the result to TOOL_INFOS.\n",
    "# Example:\n",
    "# VECTOR_SEARCH_TOOLS.append(\n",
    "#     VectorSearchRetrieverTool(\n",
    "#         index_name=\"\",\n",
    "#         # filters=\"...\"\n",
    "#     )\n",
    "# )\n",
    "\n",
    "tools.extend(VECTOR_SEARCH_TOOLS)\n",
    "\n",
    "#####################\n",
    "## Define agent logic\n",
    "#####################\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    custom_inputs: Optional[dict[str, Any]]\n",
    "    custom_outputs: Optional[dict[str, Any]]\n",
    "    user_id: Optional[str]\n",
    "\n",
    "\n",
    "class LangGraphResponsesAgent(ResponsesAgent):\n",
    "    \"\"\"Stateless agent using ResponsesAgent with user-based long-term memory.\n",
    "\n",
    "    Features:\n",
    "    - Connection pooling with credential rotation via DatabricksStore\n",
    "    - User-based long-term memory persistence (memories stored under \"users\".user_id) in \"store\" table\n",
    "    - Tool support with UC functions\n",
    "    - Automatic connection management - borrows connections per operation for scalability\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.lakebase_instance_name = LAKEBASE_INSTANCE_NAME\n",
    "        self.system_prompt = SYSTEM_PROMPT\n",
    "        self.model = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "        self._store = None\n",
    "        self._memory_tools = None\n",
    "\n",
    "    @property\n",
    "    def store(self):\n",
    "        \"\"\"Lazy initialization of DatabricksStore with semantic search support.\"\"\"\n",
    "        if self._store is None:\n",
    "            logger.info(f\"Initializing DatabricksStore with instance: {self.lakebase_instance_name} and embedding endpoint {EMBEDDING_ENDPOINT} with dims {EMBEDDING_DIMS}\")\n",
    "            self._store = DatabricksStore(\n",
    "                instance_name=self.lakebase_instance_name,\n",
    "                embedding_endpoint=EMBEDDING_ENDPOINT,\n",
    "                embedding_dims=EMBEDDING_DIMS,\n",
    "            )\n",
    "            self._store.setup()\n",
    "        return self._store\n",
    "\n",
    "    @property\n",
    "    def memory_tools(self):\n",
    "        \"\"\"Lazy initialization of memory tools.\"\"\"\n",
    "        if self._memory_tools is None:\n",
    "            logger.info(\"Creating memory tools\")\n",
    "            self._memory_tools = self._create_memory_tools()\n",
    "        return self._memory_tools\n",
    "\n",
    "    @property\n",
    "    def model_with_all_tools(self):\n",
    "        all_tools = tools + self.memory_tools\n",
    "        return self.model.bind_tools(all_tools) if all_tools else self.model\n",
    "\n",
    "    def _create_memory_tools(self):\n",
    "        \"\"\"Create tools for reading and writing long-term memory.\"\"\"\n",
    "\n",
    "        @tool\n",
    "        def get_user_memory(query: str, config: RunnableConfig) -> str:\n",
    "            \"\"\"Search for relevant information about the user from long-term memory using semantic search via vector embeddings.\n",
    "\n",
    "            Use this tool to retrieve previously saved information about the user,\n",
    "            such as their preferences, facts they've shared, or other personal details.\n",
    "\n",
    "            Args:\n",
    "            \"\"\"\n",
    "            user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
    "            if not user_id:\n",
    "                return \"Memory not available - no user_id provided.\"\n",
    "\n",
    "            namespace = (\"user_memories\", user_id.replace(\".\", \"-\"))\n",
    "\n",
    "            results = self.store.search(namespace, query=query, limit=5)\n",
    "\n",
    "            if not results:\n",
    "                return \"No memories found for this user.\"\n",
    "\n",
    "            memory_items = []\n",
    "            for item in results:\n",
    "                memory_items.append(f\"- [{item.key}]: {json.dumps(item.value)}\")\n",
    "\n",
    "            return f\"Found {len(results)} relevant memories (ranked by semantic similarity):\\n\" + \"\\n\".join(memory_items)\n",
    "\n",
    "        @tool\n",
    "        def save_user_memory(memory_key: str, memory_data_json: str, config: RunnableConfig) -> str:\n",
    "            \"\"\"Save information about the user to long-term memory with vector embeddings.\n",
    "\n",
    "            Use this tool to remember important information the user shares about themselves,\n",
    "            such as preferences, facts, or other personal details.\n",
    "\n",
    "            Args:\n",
    "                memory_key: A descriptive key for this memory (e.g., \"preferences\", \"favorite_color\", \"background_info\")\n",
    "                memory_data_json: JSON string with the information to remember.\n",
    "                    Example: '{\"favorite_color\": \"purple\"}'\n",
    "            \"\"\"\n",
    "            user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
    "            if not user_id:\n",
    "                return \"Cannot save memory - no user_id provided.\"\n",
    "\n",
    "            namespace = (\"user_memories\", user_id.replace(\".\", \"-\"))\n",
    "\n",
    "            try:\n",
    "                memory_data = json.loads(memory_data_json)\n",
    "                # Validate that memory_data is a dictionary (not a list or other type)\n",
    "                if not isinstance(memory_data, dict):\n",
    "                    return f\"Failed to save memory: memory_data must be a JSON object (dictionary), not {type(memory_data).__name__}. Example: '{{\\\"key\\\": \\\"value\\\"}}'\"\n",
    "                self.store.put(namespace, memory_key, memory_data)\n",
    "                return f\"Successfully saved memory with key '{memory_key}' for user.\"\n",
    "            except json.JSONDecodeError as e:\n",
    "                return f\"Failed to save memory: Invalid JSON format - {str(e)}\"\n",
    "\n",
    "        @tool\n",
    "        def delete_user_memory(memory_key: str, config: RunnableConfig) -> str:\n",
    "            \"\"\"Delete a specific memory from the user's long-term memory.\n",
    "\n",
    "            Use this tool when the user asks you to forget something or remove\n",
    "            a piece of information from their memory.\n",
    "\n",
    "            Args:\n",
    "                memory_key: The key of the memory to delete (e.g., \"preferences\", \"likes\", \"background_info\")\n",
    "            \"\"\"\n",
    "            user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
    "            if not user_id:\n",
    "                return \"Cannot delete memory - no user_id provided.\"\n",
    "\n",
    "            namespace = (\"user_memories\", user_id.replace(\".\", \"-\"))\n",
    "\n",
    "            self.store.delete(namespace, memory_key)\n",
    "            return f\"Successfully deleted memory with key '{memory_key}' for user.\"\n",
    "\n",
    "        return [get_user_memory, save_user_memory, delete_user_memory]\n",
    "\n",
    "    def _create_graph(self):\n",
    "        \"\"\"Create the LangGraph workflow\"\"\"\n",
    "        def should_continue(state: AgentState):\n",
    "            messages = state[\"messages\"]\n",
    "            last_message = messages[-1]\n",
    "            if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "                return \"continue\"\n",
    "            return \"end\"\n",
    "\n",
    "        model_with_tools = self.model_with_all_tools\n",
    "\n",
    "        if self.system_prompt:\n",
    "            preprocessor = RunnableLambda(\n",
    "                lambda state: [{\"role\": \"system\", \"content\": self.system_prompt}] + state[\"messages\"]\n",
    "            )\n",
    "        else:\n",
    "            preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "\n",
    "        model_runnable = preprocessor | model_with_tools\n",
    "\n",
    "        def call_model(state: AgentState, config: RunnableConfig):\n",
    "            response = model_runnable.invoke(state, config)\n",
    "            return {\"messages\": [response]}\n",
    "\n",
    "        workflow = StateGraph(AgentState)\n",
    "        workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "\n",
    "        active_tools = (tools + self.memory_tools)\n",
    "\n",
    "        if active_tools:\n",
    "            workflow.add_node(\"tools\", ToolNode(active_tools))\n",
    "            workflow.add_conditional_edges(\n",
    "                \"agent\",\n",
    "                should_continue,\n",
    "                {\"continue\": \"tools\", \"end\": END}\n",
    "            )\n",
    "            workflow.add_edge(\"tools\", \"agent\")\n",
    "        else:\n",
    "            workflow.add_edge(\"agent\", END)\n",
    "\n",
    "        workflow.set_entry_point(\"agent\")\n",
    "\n",
    "        return workflow.compile()\n",
    "\n",
    "    def _get_user_id(self, request: ResponsesAgentRequest) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Use user_id from chat context if available, return None if not provided\n",
    "        \"\"\"\n",
    "        # User id from chat context as user id to store memories\n",
    "        # https://mlflow.org/docs/latest/api_reference/python_api/mlflow.types.html#mlflow.types.agent.ChatContext\n",
    "        if request.context and getattr(request.context, \"user_id\", None):\n",
    "            return request.context.user_id\n",
    "        return None\n",
    "\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        \"\"\"Non-streaming prediction\"\"\"\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        request: ResponsesAgentRequest,\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"Streaming prediction\"\"\"\n",
    "        user_id = self._get_user_id(request)\n",
    "\n",
    "        # If there is no user_id, we cannot retrieve memories\n",
    "        if not user_id:\n",
    "            logger.error(\n",
    "                \"Cannot store or retrieve memories without a user_id.\"\n",
    "            )\n",
    "\n",
    "        ci = dict(request.custom_inputs or {})\n",
    "        if user_id:\n",
    "            ci[\"user_id\"] = user_id\n",
    "        request.custom_inputs = ci\n",
    "\n",
    "        cc_msgs = to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "\n",
    "        run_config = {\"configurable\": {}}\n",
    "        if user_id:\n",
    "            run_config[\"configurable\"][\"user_id\"] = user_id\n",
    "\n",
    "        graph = self._create_graph()\n",
    "\n",
    "        state_input = {\"messages\": cc_msgs}\n",
    "        if user_id:\n",
    "            state_input[\"user_id\"] = user_id\n",
    "\n",
    "        # Stream the graph execution\n",
    "        for event in graph.stream(\n",
    "            state_input,\n",
    "            run_config,\n",
    "            stream_mode=[\"updates\", \"messages\"]\n",
    "        ):\n",
    "            if event[0] == \"updates\":\n",
    "                for node_data in event[1].values():\n",
    "                    if len(node_data.get(\"messages\", [])) > 0:\n",
    "                        yield from output_to_responses_items_stream(node_data[\"messages\"])\n",
    "            # Stream message chunks for real-time text generation\n",
    "            elif event[0] == \"messages\":\n",
    "                try:\n",
    "                    chunk = event[1][0]\n",
    "                    if isinstance(chunk, AIMessageChunk) and (content := chunk.content):\n",
    "                        yield ResponsesAgentStreamEvent(\n",
    "                            **self.create_text_delta(delta=content, item_id=chunk.id),\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error streaming chunk: {e}\")\n",
    "\n",
    "# ----- Export model -----\n",
    "mlflow.langchain.autolog()\n",
    "AGENT = LangGraphResponsesAgent()\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "417a76af-dbb6-4e9e-bdc3-6e80e1ad5e3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Test the Agent locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "897d62f7-34bc-4f64-b7f2-f113bf4e5c2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc69aa4-e107-4b6b-838a-d99a00580f5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# example using user_id from ChatContext as input user_id\n",
    "# https://mlflow.org/docs/latest/api_reference/python_api/mlflow.types.html#mlflow.types.agent.ChatContext\n",
    "from agent import AGENT\n",
    "import mlflow\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ChatContext\n",
    ")\n",
    "\n",
    "req = ResponsesAgentRequest(\n",
    "    input=[{\"role\": \"user\", \"content\": \"Please remember I use Databricks and I am a python developer who likes pistachios and has a dog named Fluffy\"}],\n",
    "    context=ChatContext(\n",
    "        conversation_id=\"abc\",\n",
    "        user_id=\"email@databricks.com\"\n",
    "    ),\n",
    ")\n",
    "result = AGENT.predict(req)\n",
    "\n",
    "print(result.model_dump(exclude_none=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6d7df42-c207-4d1d-b7d6-c85afe87c899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Recall memory example\n",
    "\n",
    "req = ResponsesAgentRequest(\n",
    "    input=[{\"role\": \"user\", \"content\": \"What data platform do I use?\"}],\n",
    "    context=ChatContext(\n",
    "        conversation_id=\"abc\",\n",
    "        user_id=\"email@databricks.com\"\n",
    "    ),\n",
    ")\n",
    "result = AGENT.predict(req)\n",
    "\n",
    "print(result.model_dump(exclude_none=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcebb9ad-1702-485b-9b66-0c5c1e6f5a2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Log the agent as an MLflow model\n",
    "Log the agent as code from the agent.py file. See [MLflow - Models from Code](https://mlflow.org/docs/latest/models.html#models-from-code).\n",
    "\n",
    "## Enable automatic authentication for Databricks resources\n",
    "For the most common Databricks resource types, Databricks supports and recommends declaring resource dependencies for the agent upfront during logging. This enables automatic authentication passthrough when you deploy the agent. With automatic authentication passthrough, Databricks automatically provisions, rotates, and manages short-lived credentials to securely access these resource dependencies from within the agent endpoint.\n",
    "\n",
    "To enable automatic authentication, specify the dependent Databricks resources when calling `mlflow.pyfunc.log_model()`.\n",
    "\n",
    "**TODO:** \n",
    "- Add lakebase as a resource type\n",
    "- If your Unity Catalog tool queries a [vector search index](https://docs.databricks.com/docs%20link) or leverages [external functions](https://docs.databricks.com/docs%20link), you need to include the dependent vector search index and UC connection objects, respectively, as resources. See docs ([AWS](https://docs.databricks.com/generative-ai/agent-framework/log-agent.html#specify-resources-for-automatic-authentication-passthrough) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/log-agent#resources))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a28395e3-a434-44e3-850c-4da59df36e8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Determine Databricks resources to specify for automatic auth passthrough at deployment time\n",
    "import mlflow\n",
    "from agent import tools, LLM_ENDPOINT_NAME, LAKEBASE_INSTANCE_NAME\n",
    "from databricks_langchain import VectorSearchRetrieverTool\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint, DatabricksLakebase\n",
    "from unitycatalog.ai.langchain.toolkit import UnityCatalogTool\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "resources = [DatabricksServingEndpoint(LLM_ENDPOINT_NAME), DatabricksLakebase(database_instance_name=LAKEBASE_INSTANCE_NAME)]\n",
    "\n",
    "for tool in tools:\n",
    "    if isinstance(tool, VectorSearchRetrieverTool):\n",
    "        resources.extend(tool.resources)\n",
    "    elif isinstance(tool, UnityCatalogTool):\n",
    "        resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
    "\n",
    "input_example = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is an LLM agent?\"\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "logged_agent_info = mlflow.pyfunc.log_model(\n",
    "    name=\"agent\",\n",
    "    python_model=\"agent.py\",\n",
    "    input_example=input_example,\n",
    "    resources=resources,\n",
    "    pip_requirements=[\n",
    "        \"mlflow==3.6.0\",\n",
    "        f\"databricks-langchain[memory]=={get_distribution('databricks-langchain[memory]').version}\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b69a782f-e0b8-4e66-be7e-4dcd70f2e142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Evaluate the agent with Agent Evaluation\n",
    "Use Mosaic AI Agent Evaluation to evalaute the agent's responses based on expected responses and other evaluation criteria. Use the evaluation criteria you specify to guide iterations, using MLflow to track the computed quality metrics. See Databricks documentation ([AWS](https://docs.databricks.com/(https://docs.databricks.com/aws/generative-ai/agent-evaluation) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/agent-evaluation/)).\n",
    "\n",
    "To evaluate your tool calls, add custom metrics. See Databricks documentation ([AWS](https://docs.databricks.com/en/generative-ai/agent-evaluation/custom-metrics.html#evaluating-tool-calls) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-evaluation/custom-metrics#evaluating-tool-calls))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be48cb00-1163-4d53-8ae3-bbcca7602d60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import RelevanceToQuery, RetrievalGroundedness, RetrievalRelevance, Safety\n",
    "\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\"input\": [{\"role\": \"user\", \"content\": \"Calculate the 15th Fibonacci number\"}]},\n",
    "        \"expected_response\": \"The 15th Fibonacci number is 610.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=lambda input: AGENT.predict({\"input\": input}),\n",
    "    scorers=[RelevanceToQuery(), Safety()],  # add more scorers here if they're applicable\n",
    ")\n",
    "\n",
    "# Review the evaluation results in the MLfLow UI (see console output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1de25b6-0055-42ca-9efd-97d2c9c2f649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pre-deployment agent validation\n",
    "Before registering and deploying the agent, perform pre-deployment checks using the mlflow.models.predict() API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a7e4d9c-a32c-427c-97d7-c46d6ca28904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.models.predict(\n",
    "    model_uri=logged_agent_info.model_uri,\n",
    "    input_data={\"input\": [{\"role\": \"user\", \"content\": \"I am working on stateful agents\"}]},\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3bacd37-89d4-4ae7-9582-a799f116a5a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Register the model to Unity Catalog\n",
    "Update the `catalog`, `schema`, and `model_name` below to register the MLflow model to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bff8902-06f0-4f5b-aa1e-7be2c80e744e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "catalog = \"catalog\"\n",
    "schema = \"schema\"\n",
    "model_name = \"long-term-memory-agent\"\n",
    "\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5133307e-20a2-43dc-bdae-ad28538de9dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Deploy the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8e0a584-ff89-4142-903c-f73ed287d0f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "agents.deploy(UC_MODEL_NAME, uc_registered_model_info.version, tags = {\"endpointSource\": \"docs\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bac0db6f-5d36-4633-bf6b-b5fdbd1ac227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Next steps\n",
    "It will take around 15 minutes for you to finish deploying your agent. After your agent is deployed, you can chat with it in AI playground to perform additional checks, share it with SMEs in your organization for feedback, or embed it in a production application. \n",
    "\n",
    "Now, with your stateful agent, you can pick up past threads and continue the conversation.\n",
    "\n",
    "You can query your Lakebase instance to see a record of your user memories. Here is a basic query to see items in the store:\n",
    "```\n",
    "select *\n",
    "from public.store\n",
    "order by updated_at desc\n",
    "limit 50;\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "long-term-memory-agent-lakebase",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
